{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) In dieser Aufgabe soll nun ein kleines Neuronales Netz aus fünf Neuronen zur Erkennung eines Schachbrettmusters auf einem 2x2 Feld programmiert werden. Als Eingabe für das 2x2 Feld nutzen wir einen Vektor mit 4 Einträgen, wobei eine 1 ein schwarzes Feld und eine 0 ein weißes Feld darstellt.\n",
        "Zu Beginn müssen wir die Klassen HHSimulationOdeint, HHModel und HHParameters aus Aufgabe 2 importieren. Wie bereits gezeigt, ist die odeint Methode die genaueste und schnellste Methode zum Berechnen der GDG und wird fortan verwendet.\n",
        "Weiter enthält der Code in den folgenden Zeilen eine Methode zum Zeichnen des Schachbretts, die für ein vereinfachtes Verständnis sorgen soll."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'eingangsvektor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m good_pattern \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Zeichne das Schachbrett\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mzeichne_schachbrett\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgood_pattern\u001b[49m\u001b[43m)\u001b[49m \n",
            "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mzeichne_schachbrett\u001b[1;34m(vektor)\u001b[0m\n\u001b[0;32m     15\u001b[0m schachbrett \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vektor)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Vektor in String für Title der Grafik\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m vector_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[43meingangsvektor\u001b[49m))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Erstellt das Schachbrett\u001b[39;00m\n\u001b[0;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(schachbrett, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray_r\u001b[39m\u001b[38;5;124m'\u001b[39m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'eingangsvektor' is not defined"
          ]
        }
      ],
      "source": [
        "import nbimporter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Aufgabe_2 import HHSimulationOdeint, HHModel, HHParameters\n",
        "\n",
        "# Methode zum Zeichnen des Schachbretts\n",
        "def zeichne_schachbrett(vektor):\n",
        "    \n",
        "    # Wirft Fehler, wenn nicht 1 oder 0, da sonst Graustufen erlaubt wären oder Länge des Vektor ungleich 4\n",
        "    if len(vektor) != 4 or any(bit not in [0, 1] for bit in vektor):\n",
        "        raise ValueError(\"Der Vektor muss genau vier Zahlen enthalten, die 0 oder 1 sein können.\")\n",
        "\n",
        "    # Erstellt ein 2x2-Array \n",
        "    schachbrett = np.array(vektor).reshape(2, 2)\n",
        "    \n",
        "    # Vektor in String für Title der Grafik\n",
        "   \n",
        "    vector_str = ', '.join(map(str, vektor))\n",
        "    \n",
        "    # Erstellt das Schachbrett\n",
        "    plt.imshow(schachbrett, cmap='gray_r', vmin=0, vmax=1)\n",
        "    plt.xticks([])  # Entferne x-Achsenmarkierungen\n",
        "    plt.yticks([])  # Entferne y-Achsenmarkierungen\n",
        "    plt.title(f'Schachbrett, das als richtig erkannt werden soll')\n",
        "    plt.show()\n",
        "\n",
        "good_pattern = [1, 0, 0, 1]\n",
        "\n",
        "# Zeichne das Schachbrett\n",
        "zeichne_schachbrett(good_pattern) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "params = HHParameters()\n",
        "hh_model = HHModel(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wir geben zur Implementierung der Trainingsdaten verschiedene Vektoren vor, die die unterschiedlichen möglichen Muster auf dem 2x2 Brett darstellen, in possible_patters vor. Dabei ist zu beachten, dass jedes Brett immer aus zwei weißen und zwei schwarzen Feldern besteht. good_pattern enthält den Vektor, den wir als korrekt erkennen wollen. Im gegebenen Fall ist dies das Brett, welches oben links und unten rechts ein schwarzes Feld aufweist.\n",
        "Da das Neuronale Netz aus 5 Neuronen, also vier Input-Neuronen (je eins für jedes Feld auf dem Brett) und einem Output-Neuron (Muster vorhanden/Muster nicht vorhanden) bestehen soll, ist es nur möglich eine der beiden Diagonalen als richtiges Muster zu erkennen. Die optimalen Wichtungsfaktoren (diese sollen Werte zwischen 0 und 1 annehmen) zur Erkennung des Musters sind 1 für die gewünschten schwarzen Felder, also oben links und unten rechts und 0 auf den anderen beiden Feldern. \n",
        "\n",
        "Da ein guter Trainingsdatensatz etwa zur Hälfte aus dem richtigen Muster und zur anderen Hälfte aus falschen Mustern bestehen sollte, lassen wir den Trainingsdatensatz zu 40% aus dem good_pattern und zu 60% aus einem zufällig geshuffleten Vektor aus zwei Einsen und zwei Nullen bestehen. Der Trainingsdatensatz enthält 25 Muster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "possible_patterns = [[1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1], [0, 1, 1, 0], [0, 1, 0, 1], [0, 0, 1, 1]]\n",
        "good_pattern = [1, 0, 0, 1]\n",
        "\n",
        "training_data_length = 25\n",
        "training_data = np.zeros((training_data_length, 4), dtype=int)\n",
        "\n",
        "# updates the global training_data variable with new patterns\n",
        "def make_new_training_data():\n",
        "    for i in range(training_data_length):\n",
        "        pattern = np.array([1, 1, 0, 0])\n",
        "        \n",
        "        # extra 40% chance of using the good pattern, so that the network can learn to recognize the good pattern\n",
        "        if np.random.rand() < 0.4:\n",
        "            pattern = good_pattern\n",
        "        else: \n",
        "            np.random.shuffle(pattern)\n",
        "\n",
        "        training_data[i] = pattern\n",
        "\n",
        "make_new_training_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nachdem der Satz für die Trainingsdaten initialisert wurde, kann nun die Klasse für das Neuronale Netz geschrieben werden. \n",
        "Die Wichtigunsfaktoren werden in einem Vektor Länge 4 dargestellt (Annahme für optimalen Vektor der Wichtungsfaktoren: [1, 0, 0, 1]) und per Skalarprodukt mit dem Vektor, der das Muster darstellt, verrechnet. \n",
        "Wenn man die die Wichtungsfaktoren  als Skalarprodukt mit dem Vektor für das als richtig zu erkennende Muster nimmt, erhalten wir 2 als Ergebniss, während ein Vektor, der nicht dem gesuchten Muster entspricht, eine 1 oder 0 als Ergbniss ausgeben würde.  \n",
        "Dieses Ergbenis wird mit dem extern anliegenden Strom und dem Faktor 6 multipliziert und als ein sekündiger Inputstromimpuls für das HHModel genommen. So ist sichergestellt, dass nur bei einem richtigen Muster ein Ausschlag in der Spannung des HHModels entsteht. Der maximale Wert der Spannung U wird durch eine Sigmoidfunktion transformiert, um eine Entscheidung zu treffen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class NN_5_Neurons:\n",
        "\n",
        "    # weights is a vector of length 4\n",
        "    def __init__(self, weights):\n",
        "        self.weights = np.array(weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def getInputCurrent(weightedSum):\n",
        "        def I_ext(t):\n",
        "            return  6 * weightedSum if t < 1 else -5\n",
        "        return I_ext\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def is14DiagonalBlack(self, pattern):\n",
        "        weightedSum = np.dot(self.weights, pattern)\n",
        "        I_ext = self.getInputCurrent(weightedSum)\n",
        "\n",
        "        _, U, _, _, _ = HHSimulationOdeint(hh_model, time=10, dt=0.01, I_ext=I_ext).run()\n",
        "\n",
        "        return self.sigmoid(max(U) / 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.c) Zum Test sollen alle Wichtungsfaktoren auf 1 gesetzt werden (entspricht dem Vektor [1, 1, 1, 1]) und das Ergebnis mit den von uns als gut angenommenen Wichtungsfaktoren [1, 0, 0, 1] verglichen werden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# good weights to recognize the pattern 'good_pattern' \n",
        "good_weights = np.copy(good_pattern)\n",
        "\n",
        "nn_ones = NN_5_Neurons(weights=[1, 1, 1, 1])\n",
        "results_ones = [nn_ones.is14DiagonalBlack(pattern) for pattern in possible_patterns]\n",
        "\n",
        "print(f\"results for NN with 1 as all weights: {results_ones}\")\n",
        "\n",
        "nn_good = NN_5_Neurons(good_weights)\n",
        "results_good = [nn_good.is14DiagonalBlack(pattern) for pattern in possible_patterns]\n",
        "\n",
        "print(f\"results for NN with good weights: {results_good}\")\n",
        "\n",
        "print(f\"perfect results: {[1 if pattern == good_pattern else 0 for pattern in possible_patterns]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Für ein als richtig erkanntes Muster soll dabei einen Wert nahe 1 ausgebene werden, während ein als falsch erkanntes Muster einem Wert gegen 0 entspricht.\n",
        "Es ist zu erkennen, dass wenn alle Wichtungsfaktoren auf 1 gesetzt sind, jedes Muster als \"richtig\" erkannt werden würde auch wenn es eigentlich ein falsches Muster ist. Dies entspricht den Erwartungen, da das Ergebnis des Skalarproduktest bei einem Vektor für die Wichtungsfaktoren von [1, 1, 1, 1] immer 2 ergibt.\n",
        "Schaut man allerdings auf die Ergebnisse für die für gut angenommenen Wichtungsfaktoren, wird jedes Ergebnis mit einer Abweichung von maximal 0,0025 von dem gewünschten Ergebnis als richtig erkannt wird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.d)\n",
        "Der Algorithmus zum maschinellen Lernen soll geschrieben werden. Dies ist eine einfache Implementation des Konzeptes der Backpropergation.\n",
        "Dafür werden zuerst die Wichtungsfaktoren zufällig zwischen 0 und 1 gewählt.\n",
        "\n",
        "Die Wichtungsfaktoren und der Loss werden in zwei Arrays während des Trainings zwischengespeichert.\n",
        "Es wird ein zufälliges Muster vorgegeben und das Neuronale Netz errechnet ein Ergebnis zwischen 0 und 1. \n",
        "Der Loss stellt dabei die Abweichung des Ergebnisses vom zu erwartenden, richtigen Ergebnis (1 oder 0) für das entsprechende Pattern dar. \n",
        "Der Vektor für die Wichtungsfaktoren wird dann während des Trainings nach jedem Muster um den Loss multipliziert mit dem Vektor, der das Muster angibt, addiert. Es werden also immer nur die Wichtungsfaktoren für ein schwarzes Feld angepasst.\n",
        "Bei einem positiven Loss (falsches Pattern wird als richtig erkannt) werden die entsprechenden Gewichtungsfaktoren also um dem den Loss erhöht. Sollte ein negativer Loss (richtiges Ergebnis wird als falsch erkannt) berrechnet worden sein, werden die Gewichtungsfaktoren entsprechend verringert. \n",
        "Dabei ist zu beachten, dass niemals die Gewichte 2 und 3 erhöht werden können. Es kann allerdings passieren, dass die Gewichtungsfaktoren für das erste und vierte Neuron verringert werden. Wenn beispielsweise der Vektor [1, 1, 0, 0] fälschlicherweise als richtig erkannt wird, werden dementsprechend Wichtungsfaktor eins und zwei mit dem berechneten Loss multipliziert und um den Betrag verringert. \n",
        "So wird dafür gesorgt, dass die Wichtungsfaktoren bei einer falschen Vorhersage solange angepasst werden bis das NN die Ergebnisse genau vorhersagt. \n",
        "\n",
        "Für das Training wird 10 mal mit unterschiedlichen Start-Wichtungsfaktoren gestartet und durchläuft je 25 Iterationen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# loss_for_single_pattern can be positive and negative, corresponding to undershooting and overshooting the correct result,\n",
        "# this will tell if the weights have to be increased or decreased in training\n",
        "def loss_for_single_pattern(weights, pattern):\n",
        "    nn = NN_5_Neurons(weights)\n",
        "    return np.array_equal(pattern, good_pattern) - nn.is14DiagonalBlack(pattern)\n",
        "\n",
        "# loss_total > 0 \n",
        "def loss_total(weights):\n",
        "    return sum(abs(np.array([loss_for_single_pattern(weights, pattern) for pattern in possible_patterns])))\n",
        "\n",
        "\n",
        "def training():\n",
        "    weights = np.random.rand(4)\n",
        "\n",
        "    # we record the total loss and the weights after each training step:\n",
        "    losses_during_training = np.array([0.] * training_data_length)\n",
        "    weights_during_training = np.array([weights] * training_data_length)\n",
        "\n",
        "    for i, pattern in enumerate(training_data):\n",
        "        loss = loss_for_single_pattern(weights, pattern)\n",
        "\n",
        "        losses_during_training[i] = loss_total(weights)\n",
        "        weights_during_training[i] = weights\n",
        "\n",
        "        # the weights that contributed to the loss are the ones that got 1 as input\n",
        "        # those weights get decreased or increased depending on loss < 0 or loss > 0\n",
        "        # the change in those weights is proportional to the absolute value of the loss \n",
        "        weights += loss * pattern\n",
        "        weights = np.clip(weights, 0, 1)\n",
        "\n",
        "    return losses_during_training, weights_during_training\n",
        "\n",
        "\n",
        "biggest_final_loss = 0\n",
        "\n",
        "for i in range(10):\n",
        "    losses, weights = training()\n",
        "\n",
        "    biggest_final_loss = max(biggest_final_loss, losses[-1])\n",
        "\n",
        "    # we make new random training data to demonstrate robustness to changes in the training data set\n",
        "    make_new_training_data()\n",
        "\n",
        "    # Plot der Entwicklung des losses und der weights\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Weights\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title(f\"Training number {i + 1}:\\nStarting weights: {list(np.round(weights[0], 3))}\\nFinal loss: {losses[-1]}\")\n",
        "\n",
        "    plt.plot(weights[:, 0], label=\"w1\")\n",
        "    plt.plot(weights[:, 1], label=\"w2\")\n",
        "    plt.plot(weights[:, 2], label=\"w3\")\n",
        "    plt.plot(weights[:, 3], label=\"w4\")\n",
        "\n",
        "    plt.ylabel(\"Weights\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(losses, label=\"Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(f\"biggest loss after training: {biggest_final_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es zeigt sich, dass nach dem Training von dem Neuronalen Netz mit maximalen Abweichung von 0,035 ein richtiges von einem falschen Muster unterschieden können.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
